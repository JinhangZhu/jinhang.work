---
title: "Review: YOLOv3"
subtitle: "ç»å…¸çš„one-stageç›®æ ‡æ£€æµ‹ç®—æ³•"
date: 2020-04-08T22:18:15+01:00
categories: [Tech,"Reviews"]
tags: [YOLOv3,Object detection]
slug: "review-yolov3"
displayCopyright: true
---

YOLOv3ä»å‰ä¸¤ä»£å‘å±•è€Œæ¥ï¼Œèåˆäº†è€å¼çš„YOLOç³»åˆ—çš„one-stageçš„ç‰¹ç‚¹å’Œä¸€äº›SOTAæ¨¡å‹çš„tricksã€‚è¦äº†è§£YOLOv3ï¼Œæœ€å¥½æ˜¯å…ˆè¯»YOLOv1ä¸­å…³äºå›å½’æ¦‚å¿µå’ŒæŸå¤±å‡½æ•°çš„æè¿°ï¼ŒYOLOv2åŸºæœ¬æœ‰äº†YOLOv3çš„å½¢çŠ¶ä½†æ˜¯è¿˜æ²¡æœ‰ResNetçš„æ€æƒ³ï¼Œä¸å¦‚ç›´æ¥çœ‹YOLOv3ã€‚YOLOv3æ¨¡å‹æœ‰è¿™å‡ å¤§é‡ç‚¹ï¼š

**ç‹¬ç‰¹çš„ç½‘ç»œç»“æ„Darknet-53**ã€‚èåˆäº†BNï¼ŒResNetçš„shortcutï¼Œå¤šå°ºåº¦feature mapçš„æ‹¼æ¥ï¼›

**å¤šå°ºåº¦çš„è¾“å‡º**ã€‚è¾“å‡ºæ˜¯å…·æœ‰ä¸‰ä¸ªå°ºåº¦çš„ç‰¹å¾å›¾çš„æ‹¼æ¥ï¼Œå‰ä»£çš„å°ç›®æ ‡æ£€æµ‹çš„å¼±ç‚¹å¾—åˆ°äº†å…‹æœï¼›

**SOTAçš„é”šæ¡†æ–¹æ³•**ã€‚è™½ç„¶æ˜¯YOLOv2å°±æœ‰çš„ï¼Œä½†æ˜¯YOLOv3åšäº†æ”¹è¿›ï¼Œanchor-basedçš„æ–¹æ³•æˆå…¨äº†YOLOv3ï¼›

**ç²¾å¿ƒè®¾è®¡çš„æŸå¤±å‡½æ•°**ã€‚æ²¿ç”¨YOLOv1å¯¹æŸå¤±å‡½æ•°çš„å®šä¹‰ï¼Œç¨ä½œè°ƒæ•´ã€‚

<!--more-->

## README

- è¿™ç¯‡æ–‡ç« åœ¨è‹±æ–‡ç‰ˆçš„reviewåŸºç¡€ä¸Šä¿®æ”¹å¾—æ¥ï¼Œå†™çš„æ—¶å€™è§‰å¾—ä¹‹å‰reviewçš„æ˜¯ä¸ªä»€ä¹ˆä¸œè¥¿ï¼Œé‡ç‚¹åªæŠŠæ¡ä¸€éƒ¨åˆ†ï¼Œæ²¡æœ‰è®¤è¯†åˆ°ç½‘ç»œç»“æ„å’ŒæŸå¤±å‡½æ•°çš„é‡è¦æ€§ã€‚æ‰€ä»¥æœ€åæ–‡ç« æŠŠåŸæ–‡æ”¾åº•ä¸‹ï¼Œç»†èŠ‚ç®—æ–°å†™çš„ã€‚åŸæ–‡è¿˜æ˜¯å¯ä»¥çœ‹çš„ï¼Œå½“ä½œç†Ÿæ‚‰æ•´ä½“æ€è·¯ï¼Œä»¥åŠçœ‹YOLOv3çš„æµ‹è¯•è¡¨ç°çš„æè¿°ã€‚
- ç†è§£YOLOv3é‡åœ¨ç†è§£ç½‘ç»œç»“æ„çš„å±‚ï¼Œç‰¹åˆ«æ˜¯æ£€æµ‹å±‚çš„åŠŸèƒ½ã€‚
- æˆ‘æƒ³çš„æ˜¯ï¼Œä¸€å®šè¦å±•ç°YOLOv3ä¸ºä»€ä¹ˆä»YOLOv2èƒ½å˜å¼ºè¿™ä¹ˆå¤šï¼Œå› ä¸ºå®ƒä¿©å®é™…ä¸Šå¾ˆåƒï¼Œä½†æ˜¯äº›å¾®çš„è°ƒæ•´å°±å¸¦æ¥äº†å¤§çš„è¿›æ­¥ï¼ŒResNetçš„shortcutç»“æ„ä¸ºä»€ä¹ˆèƒ½å¸¦æ¥æ›´å¥½çš„ç‰¹å¾æè¿°ï¼Œè¿™äº›è¿˜è¦ä»ResNetè®²èµ·ã€‚è¯è¯´æˆ‘ä¸ºä»€ä¹ˆè¦æŠŠæ®‹å·®ç½‘ç»œå½“ä½œæˆ‘çš„ç¬¬ä¸€ç¯‡reviewçš„è®ºæ–‡...å«æ®‹æš´èµ·ç‚¹ç®—äº†ã€‚
- æ–‡ç« é‡Œçš„å¾ˆå¤šä¸œè¥¿ï¼Œæ˜¯è®ºæ–‡é‡Œæ²¡å†™åˆ°çš„ï¼Œç»†èŠ‚ä¸€èˆ¬æ¥æºäºæŒ‰ç…§å‰äººçš„from-scratchç³»åˆ—é‡å†™çš„[ä»£ç ](https://github.com/JinhangZhu/yolov3-from-scratch)ã€‚
- å¼•ç”¨å¾ˆç²¾å½©ï¼Œç”¨æ¥åšæ‰©å±•é˜…è¯»æŒºå¥½ã€‚

## ç½‘ç»œç»“æ„

è¿™é‡Œæˆ‘poå‡ºä¸¤å¼ å›¾ï¼Œç¬¬ä¸€å¼ æ˜¯ç›´è§‚çš„ï¼Œé‡åœ¨å±•ç°feature mapçš„å˜åŒ–è¿‡ç¨‹ï¼›ç¬¬äºŒå¼ æ˜¯ç»†èŠ‚çš„ï¼Œé‡åœ¨å±•ç°ç½‘ç»œçš„èµ·ä½œç”¨çš„å•å…ƒç»“æ„ã€‚

<img src="https://i.loli.net/2020/06/29/EPfZG5SoA1maiuV.jpg" title="Image credit: https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b">

<img src="https://i.loli.net/2020/06/29/SrLaZXwQHm91ODp.png" title="Image credit: https://blog.csdn.net/leviopku/article/details/82660381">

- **æ²¡æœ‰æ± åŒ–å±‚**ã€‚é‚£æ€ä¹ˆå®ç°ç‰¹å¾å›¾å°ºå¯¸å˜æ¢å‘¢ï¼Ÿæ˜¯..å·ç§¯æ ¸çš„strideçš„æ”¹å˜æ¥å®ç°..ç¼©å°çš„ã€‚YOLOv3å’ŒYOLOv2ä¸€æ ·ï¼Œè¾“å…¥ç»è¿‡äº”æ¬¡ç¼©å°ç¼©å°æ¯”ä¾‹ä¸º$1/2^5=1/32$ï¼Œæ‰€ä»¥è¦æ±‚è¾“å‡ºæ˜¯32çš„å€æ•°ã€‚å¦‚æœè¾“å…¥ä¸º416Ã—416ï¼Œåˆ™æœ€å°çš„ç‰¹å¾å›¾æ˜¯13Ã—13ï¼ˆè€ƒè™‘å±æ€§çš„ç»´åº¦åˆ™æ˜¯13Ã—13Ã—255ï¼‰çš„ã€‚
- **æ²¡æœ‰å…¨è¿æ¥å±‚**ã€‚é‚£è¾“å‡ºä¸æ˜¯scoresäº†å—ï¼Ÿå¯¹çš„ï¼Œone-stageçš„YOLOv3ï¼Œå°†è¾¹ç•Œæ¡†çš„å®šä½å’Œç›®æ ‡çš„åˆ†ç±»å½“ä½œå›å½’é—®é¢˜ï¼Œèåˆä¸ºä¸€æ­¥äº†ï¼Œè¾“å‡ºçš„tensoré‡Œï¼Œ..æ—¢åŒ…æ‹¬æ‰€æœ‰å€™é€‰çš„ä½ç½®ä¿¡æ¯çš„é¢„æµ‹ï¼Œä¹ŸåŒ…æ‹¬å¹³å¸¸æƒ…å†µä¸‹å…¨è¿æ¥å±‚æ‰€è¾“å‡ºçš„åˆ†ç±»åˆ†æ•°..ã€‚

## å¤šå°ºåº¦çš„è¾“å‡º

YOLOv3åœ¨feature mapçš„æ¯ä¸€ä¸ªæ ¼å­éƒ½ä¼šé¢„æµ‹è¾¹ç•Œæ¡†ã€‚å°†ç›®æ ‡çš„è¾¹ç•Œæ¡†çš„å®šä½å½“ä½œå›å½’é—®é¢˜åšï¼Œé€šè¿‡å‡å°é”šæ¡†å’ŒçœŸå®è¾¹ç•Œæ¡†çš„æŸå¤±å‡½æ•°ï¼Œå­¦ä¹ é¢„æµ‹é”šæ¡†ç¼©æ”¾å¤§å°å’Œåç§»ã€‚YOLOv3åœ¨ä¸€ä¸ªç‰¹å¾å›¾æ ¼å­ä¼šé¢„æµ‹ä¸‰ä¸ªå€™é€‰æ¡†ï¼Œmulti-scaleæ˜¯é€šè¿‡ä¸‰ä¸ªæ‹¼æ¥æˆçš„ä¸åŒscaleçš„ç‰¹å¾å›¾å®ç°çš„ï¼Œä¼šæœ‰å¾ˆå¤šå€™é€‰æ¡†ï¼Œè¿™ä¸‰ä¸ªscaleæ˜¯åœ¨ç½‘ç»œä¸­ä¸åŒä½ç½®å®ç°çš„ã€‚

### æ£€æµ‹å±‚

æ£€æµ‹å±‚ä¸åŒäºä¼ ç»Ÿçš„CNNä¸­çš„å±‚ï¼Œä½¿ç”¨çš„æ£€æµ‹æ ¸ä¸º1Ã—1Ã—(BÃ—(5+C))çš„ï¼ŒBæ˜¯æŒ‡ç‰¹å¾å›¾ä¸€ä¸ªæ ¼å­æ‰€èƒ½é¢„æµ‹çš„è¾¹ç•Œæ¡†æ•°ç›®ï¼Œ5è¡¨ç¤ºä¸€ä¸ªè¾¹ç•Œæ¡†çš„4ä¸ªä½ç½®å±æ€§+ä¸€ä¸ªç›®æ ‡åˆ†æ•°ï¼ŒCæ˜¯æŒ‡ç±»åˆ«æ•°ç›®ã€‚åœ¨COCOè®­ç»ƒé›†ä¸Šè®­ç»ƒçš„YOLOv3é‡Œï¼ŒB=3, C=80ï¼Œæ‰€ä»¥æ£€æµ‹æ ¸å¤§å°æ˜¯1Ã—1Ã—255çš„ã€‚ä½†æ˜¯æ³¨æ„ï¼Œè™½ç„¶æ£€æµ‹å±‚æœ‰kernelçš„è®¾å®šï¼Œä½†è¿™æ ·çš„è®¾å®šæ›´ç›¸å½“äºä¸€ç§ç­‰æ•ˆï¼Œå¹¶æ²¡æœ‰è§„å®šæ˜¯æŒ‰ç…§kernelè¿ç®—çš„ï¼Œè€Œæ˜¯å°†æ£€æµ‹å±‚è¾“å…¥ï¼ˆå³å‰é¢çš„å±‚çš„è¾“å‡ºï¼‰è°ƒæ•´ç»´åº¦ï¼Œè¾“å‡ºä¸‰ç»´çš„tensorï¼Œä¹Ÿå°±æ˜¯è¯´è¿™æ ·çš„è¿ç®—å¯¼è‡´è¾“å‡ºçš„tensorä¸­..æ¯ä¸ªæ ¼å­ä¼šæœ‰255çš„æ·±åº¦..ã€‚å…·ä½“åœ°ï¼Œè¾“å…¥ç»è¿‡ç»´åº¦è°ƒæ•´ï¼Œ

```python
prediction = prediction.view(batch_size, grid_size*grid_size*num_anchors, bbox_attributes)
```

ğŸ‘†Source [permalink](https://github.com/JinhangZhu/yolov3-from-scratch/blob/d7b82df4ff64c37fb309d6d311acba4896a9e571/util.py#L115). 

- ç¬¬ä¸€ç»´è¡¨ç¤ºå›¾ç‰‡çš„IDï¼Œå¤§å°æ˜¯`batch_size`ï¼›
- ç¬¬äºŒç»´æ˜¯è¿™å¼ å›¾ç‰‡**åœ¨è¿™ä¸ªscale**çš„è¾¹ç•Œæ¡†IDï¼Œæ€»æ•°ä¸ºæ­¤æ¬¡feature mapæ‰€æœ‰çš„è¾¹ç•Œæ¡†æ•°ç›®ï¼Œå³$n_{fm}\times n_{fm}\times B$ï¼›

- ç¬¬ä¸‰ç»´æ˜¯æ‰€æœ‰çš„æŒ‰é¡ºåºé‡æ’çš„ä¸€ä¸ªè¾¹ç•Œæ¡†çš„æ‰€æœ‰å±æ€§é¡¹ï¼Œæ€»æ•°æ˜¯5+C=85=85ã€‚

<img src="https://i.loli.net/2020/06/30/vPpfikC9oxn5SMg.jpg" alt="attributes" title="Credit: https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/">



### å°ºåº¦å˜æ¢

æ–‡ç« å¼€å¤´è¯´åˆ°ï¼ŒYOLOv3çš„å°ºåº¦å˜æ¢ä¸æ˜¯é€šè¿‡æ± åŒ–å±‚å®ç°çš„ï¼Œè€Œæ˜¯å˜æ¢å·ç§¯å±‚çš„strideåšçš„ã€‚å‡è®¾è¾“å…¥å›¾ç‰‡ä¸º416Ã—416ï¼ŒYOLOv3åœ¨ä¸‰ä¸ªscaleä¸Šåšé¢„æµ‹ï¼Œé¢„æµ‹çš„ç‰¹å¾å›¾ç»“æœæŒ‰ç…§32/16/8çš„å€æ•°ä»åŸå§‹å›¾ç‰‡ä¸‹é‡‡æ ·å¾—æ¥ï¼Œå³åˆ†åˆ«ä½¿ç”¨äº†32/16/8çš„strideï¼Œé‚£ä¹ˆç»“æœçš„ç‰¹å¾å›¾å¤§å°åˆ†åˆ«ä¸º13Ã—13ï¼Œ26Ã—26ï¼Œ52Ã—52ï¼Œæ¯ä¸ªæ ¼å­é¢„æµ‹3ä¸ªè¾¹ç•Œæ¡†ï¼Œæ€»å…±ï¼š
$$
(13\times 13+26\times 26+52\times 52)\times3=10647
$$

> æ’æ’­[MTCNNå’ŒYOLOv3å¯¹æ¯”](https://jinhang.work/tech/review-mtcnn/#%E5%88%9B%E6%96%B0%E7%82%B9%E5%92%8C%E9%97%AE%E9%A2%98)

ä»¥ä¸‹ç»™å‡ºç½‘ç»œä¸­å®ç°ä¸‰ç§å°ºåº¦çš„ç‰¹å¾å›¾çš„ä½ç½®çš„Netronå›¾ç¤ºï¼š

<img src="https://i.loli.net/2020/06/30/KGMhHzBARCZo9Ut.png" alt="image-20200629200523112" title="ç¬¬ä¸€æ¬¡é¢„æµ‹ç»“æœ">



<img src="https://i.loli.net/2020/06/30/Hi7soQaW36gMDec.png" alt="image-20200629202318021" title="ç¬¬äºŒæ¬¡é¢„æµ‹ç»“æœ">

<img src="https://i.loli.net/2020/06/30/NW5yvciLfs3K4ox.png" alt="image-20200629202602747" title="ç¬¬ä¸‰æ¬¡é¢„æµ‹ç»“æœ">

## é”šæ¡†æ–¹æ³•

è¾¹ç•Œæ¡†çš„é¢„æµ‹ä½¿ç”¨åˆ°äº†ä¸Šæ–‡æ‰€è¯´çš„æ£€æµ‹å±‚ä¸‰ä¸ªå°ºåº¦ä¸Šçš„è¾“å‡ºï¼Œä»–ä»¬åŒ…æ‹¬äº†ä¸€å¹…å›¾ç‰‡ä¸‰ä¸ªå°ºåº¦æ‰€æœ‰çš„å€™é€‰ï¼Œä»¥åŠå€™é€‰çš„ä½ç½®ä¿¡æ¯ï¼Œç›®æ ‡åˆ†æ•°å’Œåˆ†ç±»åˆ†æ•°ï¼Œå³ï¼š

<img src="https://i.loli.net/2020/06/30/iRUGqXg5fp698VQ.png" alt="image-20200629213136605" title="Bounding box attributes">

**é”šæ¡†(anchors)æ˜¯ä¸€ç±»è®¾å®šå¥½çš„å…ˆéªŒæ¡†(priors)ï¼ŒYOLOv3ç½‘ç»œé¢„æµ‹å‡ºå¯¹æ•°ç©ºé—´ä¸Šçš„è½¬æ¢å…³ç³»ï¼Œæˆ–è€…è¯´æ˜¯åç§»ï¼Œå…ˆéªŒæ¡†å› æ­¤è½¬æ¢ä¸ºåéªŒæ¡†ï¼Œå³å€™é€‰ã€‚**è¿™ä¸€å¥è¯æœ‰ä¸¤ä¸ªæ¦‚å¿µï¼Œç¬¬ä¸€ä¸ªæ˜¯é”šæ¡†ï¼ŒYOLOv3å®šä¹‰äº†ä¹ä¸ªé”šæ¡†ï¼Œå®ƒä»¬æ˜¯åœ¨æ•°æ®é›†ä¸Šèšç±»å¾—æ¥çš„ï¼š

```
anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326
```

è¿™äº›æ•°å­—è¡¨ç¤ºé”šæ¡†çš„å°ºå¯¸ï¼Œåˆ†åˆ«æ˜¯å®½åº¦å’Œé«˜åº¦ã€‚æ¯ä¸€ä¸ªå°ºåº¦åªç”¨ä¸Šä¸‰ä¸ªé”šæ¡†ï¼Œå‰ä¸‰ä¸ªç”¨äºæœ€å¤§çš„ç‰¹å¾å›¾ï¼Œå³æœ€åä¸€ä¸ªæ£€æµ‹å±‚ï¼Œé€‚åˆæ£€æµ‹åŸå›¾ä¸­å°å°ºåº¦çš„ç›®æ ‡ï¼Œä¸­é—´ä¸‰ä¸ªç”¨äºä¸­é—´çš„ç‰¹å¾å›¾ï¼Œåä¸‰ä¸ªç”¨äºæœ€å°çš„ç‰¹å¾å›¾ï¼Œåˆ†åˆ«å¯¹åº”ä¸­é—´å’Œç¬¬ä¸€ä¸ªæ£€æµ‹å±‚ï¼Œé€‚åˆæ£€æµ‹åŸå›¾ä¸­ä¸­ç­‰å’Œå¤§å°ºåº¦çš„ç›®æ ‡ã€‚

ç¬¬äºŒä¸ªæ¦‚å¿µæ˜¯è½¬æ¢å…³ç³»ã€‚YOLOä¸é¢„æµ‹è¾¹ç•Œæ¡†çš„ç»å¯¹ä½ç½®ï¼Œè€Œæ˜¯ç›¸å¯¹äºæ ¼å­å·¦ä¸Šè§’çš„åç§»ã€‚åœ¨è½¬æ¢ä¹‹å‰ï¼Œæ£€æµ‹å±‚æŠŠå‰é¢çš„è¾“å‡ºå…ˆç®€å•å¤„ç†ã€‚æŠŠä¸­å¿ƒåæ ‡ç”¨sigmoidå‡½æ•°è§„å®šåˆ°0-1ä¸Šï¼Œç›®æ ‡åˆ†æ•°ä¹Ÿé€šè¿‡sigmoidå‡½æ•°æ¿€æ´»ï¼Œè¿™æ ·åšæ˜¯ä¸ºäº†è¡¨ç¤ºç›¸å¯¹äºæ ¼å­å·¦ä¸Šè§’çš„åç§»ã€‚

```python
# Sigmoid transform: centre_X, centre_Y, objectness score
prediction[:, :, 0] = torch.sigmoid(prediction[:, :, 0])
prediction[:, :, 1] = torch.sigmoid(prediction[:, :, 1])
prediction[:, :, 4] = torch.sigmoid(prediction[:, :, 4])
```

å¤„ç†åçš„ç»“æœåŒ…æ‹¬ä½ç½®ä¿¡æ¯ï¼š$t_x,t_y,t_w,t_h$ï¼Œç›®æ ‡åˆ†æ•°$p_0$ï¼Œä»¥åŠåˆ†ç±»åˆ†æ•°ã€‚é¦–å…ˆéœ€è¦æŠŠç½‘ç»œè¾“å‡ºè½¬æ¢ä¸ºè¾¹ç•Œæ¡†ä¸­å¿ƒåæ ‡$b_x,b_y$å’Œå®½åº¦$b_w$é«˜åº¦$b_h$çš„é¢„æµ‹ï¼š
$$
\begin{aligned}
b_x&=\sigma(t_x)+c_x\\\\
b_y&=\sigma(t_y)+c_y\\\\
b_w&=p_we^{t_w}\\\\
b_h&=p_he^{t_h}
\end{aligned}
$$
å…¶ä¸­$c_x,c_y$è¡¨ç¤ºç‰¹å¾å›¾æ ¼å­å·¦ä¸Šè§’çš„åæ ‡ï¼Œ$p_w,p_h$æ˜¯é”šæ¡†çš„å°ºå¯¸ã€‚

<img src="https://i.loli.net/2020/06/30/BRp5N4YzkmjZ8y3.png" title="è½¬æ¢åˆ°é¢„æµ‹æ¡†">

ä¸¾ä¸ªä¾‹å­ï¼Œå¯¹äº13Ã—13ç‰¹å¾å›¾çš„ä¸­é—´çš„æ ¼å­ï¼Œå³å·¦ä¸Šè§’åæ ‡ä¸º(6, 6)ï¼Œå¦‚æœæˆ‘ä»¬çš„ä¸­å¿ƒåæ ‡ç»è¿‡sigmoidä¹‹åä¸º(0.4, 0.7)ï¼Œé‚£ä¹ˆå¾—åˆ°æ­¤å€™é€‰åœ¨feature mapä¸Šçš„ä¸­å¿ƒåæ ‡ä¸º(6.4, 6.7)ã€‚

è‡³äºç›®æ ‡åˆ†æ•°å’Œåˆ†ç±»åˆ†æ•°ã€‚å‰è€…ç”¨äºObjectness thresholdingï¼Œåè€…ç”¨äºNMSï¼Œéƒ½æ˜¯å‡å°‘å€™é€‰çš„æ“ä½œï¼Œå°±ä¸ç»†è®²äº†ã€‚

## æŸå¤±å‡½æ•°

å‰ä»£çš„æŸå¤±å‡½æ•°å…¨éƒ¨é‡‡ç”¨Sum-squared Error Lossï¼š

<img src="https://i.loli.net/2020/06/30/5Tvx6fyOMXPikhg.png" alt="img" title="Credit: https://blog.csdn.net/qq_30159015/article/details/80446363">

- ç¬¬ä¸€è¡Œä¸ºæ€»å¹³æ–¹è¯¯å·®ï¼Œæ˜¯ä½ç½®é¢„æµ‹çš„æŸå¤±å‡½æ•°ï¼›
- ç¬¬äºŒè¡Œä¸ºæ ¹å·æ€»å¹³æ–¹è¯¯å·®ï¼Œæ˜¯å®½åº¦å’Œé«˜åº¦çš„æŸå¤±å‡½æ•°ï¼›
- ç¬¬ä¸‰ï¼Œå››è¡Œå¯¹ç›®æ ‡åˆ†æ•°æˆ–è€…ç½®ä¿¡åº¦ç”¨æ€»å¹³æ–¹è¯¯å·®ä½œä¸ºæŸå¤±å‡½æ•°ï¼›
- ç¬¬äº”è¡Œå¯¹åˆ†ç±»åˆ†æ•°ç”¨æ€»å¹³æ–¹è¯¯å·®ä½œä¸ºæŸå¤±å‡½æ•°ï¼›

YOLOv3å¯¹åä¸‰è¡Œåšäº†ä¿®æ”¹ï¼Œå°†æ€»å¹³æ–¹è¯¯å·®æ›¿æ¢ä¸ºåˆ†ç±»ä»»åŠ¡ä¸­æ›´å¥½ç”¨çš„äº¤å‰ç†µè¯¯å·®ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œ**YOLOv3ä¸­çš„ç›®æ ‡ç½®ä¿¡åº¦å’Œåˆ†ç±»åˆ†æ•°é€šè¿‡é€»è¾‘å›å½’å®ç°äº†**ã€‚åŒæ—¶ï¼Œæ¯ä¸€ä¸ªçœŸå®æ¡†ï¼ŒåªåŒ¹é…ä¸€ä¸ªè¾¹ç•Œæ¡†ï¼Œä¹Ÿå°±æ˜¯IoUæœ€å¤§çš„é‚£ä¸ªã€‚

## ç»†èŠ‚å’Œé—®é¢˜

**ä¸ç”¨softmax**ã€‚å‰ä»£çš„YOLOç½‘ç»œä½¿ç”¨äº†softmaxå¤„ç†ä¸¤ç§åˆ†æ•°ï¼Œä½†æ˜¯è¿™æ ·åšçš„å‰ææ˜¯æ•°æ®é›†çš„ç±»æ˜¯å®Œå…¨äº’æ–¥çš„ï¼Œä¹Ÿå°±æ˜¯è¯´ç›®æ ‡å¦‚æœå±äºAç±»ï¼Œå°±ä¸å¯èƒ½æ˜¯Bç±»ï¼ŒCOCOæ»¡è¶³è¿™ä¸€æ¡ä»¶ã€‚ä½†æ˜¯å¦‚æœä¸€ä¸ªæ•°æ®é›†æœ‰*Person*å’Œ*Women*è¿™æ ·çš„ç±»åˆ«ï¼Œè¿™æ ·çš„å‰æå°±ä¸æ»¡è¶³äº†ï¼Œæ‰€ä»¥YOLOv3ä½¿ç”¨äº†é€»è¾‘å›å½’æ¥é¢„æµ‹åˆ†æ•°ï¼ˆæˆ–è€…è¯´é‡‡ç”¨äº¤å‰ç†µlossï¼‰ï¼Œå¦å¤–ä½¿ç”¨thresholdingæ¥é¢„æµ‹ä¸€ä¸ªç›®æ ‡çš„å¤šä¸ªæ ‡ç­¾ï¼Œè€Œä¸æ˜¯ç”¨softmaxé€‰åˆ†æ•°æœ€å¤§çš„é‚£ä¸ªæ ‡ç­¾ã€‚

**ä½¿ç”¨äº†æ›´å¤šçš„anchor**ã€‚YOLOv2ä½¿ç”¨äº†5ä¸ªanchorsï¼Œè€ŒYOLOv3ä½¿ç”¨äº†9ä¸ªã€‚

**offsetsä¸ºä»€ä¹ˆconstrainåˆ°[0, 1]ä¸Šè€Œä¸æ˜¯[-1, 1]ä¸Š**ï¼Ÿå› ä¸ºæ˜¯é‡‡ç”¨å¯¹æ ¼å­å·¦ä¸Šè§’çš„ç›¸å¯¹åæ ‡ï¼Œä¸€ä¸ªæ ¼å­åªç®¡å³ä¸‹æ–¹çš„åæ ‡å°±è¡Œã€‚([issue](https://github.com/JinhangZhu/project-diary/issues/4#issuecomment-609789323))

**ä»è‡ªå·±çš„æ•°æ®é›†è·å–anchors**ã€‚[ç›®æ ‡æ£€æµ‹ç®—æ³•ä¹‹YOLOç³»åˆ—ç®—æ³•çš„Anchorèšç±»ä»£ç å®æˆ˜](https://zhuanlan.zhihu.com/p/95291364)ã€‚

**é¢„å¤„ç†æ­¥éª¤**ã€‚ç½‘ç»œçš„è¾“å…¥æ˜¯æ­£æ–¹å½¢çš„ï¼Œæ‰€ä»¥å¾—æŠŠå›¾ç‰‡å¤„ç†æˆæ­£æ–¹å½¢çš„ã€‚[ä»£ç ](https://github.com/JinhangZhu/yolov3-from-scratch/blob/d7b82df4ff64c37fb309d6d311acba4896a9e571/util.py#L308)æ˜¯æŠŠå›¾ç‰‡æŒ‰æ¯”ä¾‹ç¼©æ”¾ï¼ŒåµŒå…¥é•¿è¾¹ä¸€è‡´çš„æ­£æ–¹å½¢ä¸­ï¼Œå¤šä½™ä½ç½®ç”¨paddingã€‚

## å¼•ç”¨

- Redmon, J. and Farhadi, A., 2018. Yolov3: An incremental improvement. *arXiv preprint arXiv:1804.02767*.
- Redmon, Joseph, et al. "You only look once: Unified, real-time object detection." *Proceedings of the IEEE conference on computer vision and pattern recognition*. 2016.
- Redmon, Joseph, and Ali Farhadi. "YOLO9000: better, faster, stronger." *Proceedings of the IEEE conference on computer vision and pattern recognition*. 2017.
- [yoloç³»åˆ—ä¹‹yolo v3ã€æ·±åº¦è§£æã€‘](https://blog.csdn.net/leviopku/article/details/82660381)
- [ã€ç›®æ ‡æ£€æµ‹ã€‘yolov3ä¸ºä»€ä¹ˆæ¯”yolov2å¥½è¿™ä¹ˆå¤š](https://blog.csdn.net/qiu931110/article/details/81334884)
- [How to implement a YOLO (v3) object detector from scratch in PyTorch: Part 1](https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/)
- [Whatâ€™s new in YOLO v3?](https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b)

## åŸæ–‡

### Abstract

We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320 Ã— 320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 AP 50 in 51 ms on a Titan X, compared to 57.5 AP 50 in 198 ms by RetinaNet, similar performance but 3.8Ã— faster. As always, all the code is online at https://pjreddie.com/yolo/.

### Aim

There is an unfinished issue existing in the second generation: poor performance in detection of small objects or detection of a group of small objects. The author managed to solve this issue and promote accuracy while still maintaining the speed of the model. He aimed to provide the information of what he made to make YOLOv2 better and how he did it.

### Summary

The author first shared the tricks he used to improve the performance.

- **Bounding box prediction.** YOLOv3 addresses the prediction of bounding boxes as a regression problem. The loss function is the MSE. And the gradient is the ground truth value minus the prediction: $\hat{t}_b-t_b$, where $t_b$ is the tensor of the coordinates for each bounding box. The tensor is used to calculate the predictions as:

  <img src="https://i.loli.net/2020/03/30/b3Hge7WoSxIBFhs.png" alt="bbox precition.png" style="zoom:67%;" />

  The objectness score is 1 if the bounding box prior overlaps a ground truth object more than any other priors. If the box prior overlaps the ground truth by over 0.5 but is not the most, the prediction is not counted.

- **Class prediction.** Each box now predicts the classes using multilabel classification. Softmax is substituted with binary cross-entropy loss function. This modification is suitable on more complex datasets. Softmax assumes that each class is independent while some dataset contains classes which are overlapping (like Woman and Person)

- **Prediction across scales.** The author used a similar concept to SPP: adopting the feature extractor and the additional convolutional layers to encode a 3D tensor ($N\times N\times [3*(4+1+80)]$) prediction (at three scales) including the bounding box (4 coordinates), objectness (1 objectness score) and class predictions (80 classes). The algorithm takes the feature map from 2 layers previous and concatenates it with the $2\times$ unsampled feature map. This combined feature map is processed via a few more convolutional layers to predict a tensor at twice the size. The same process is applied to this tensor to create the third scaled feature map. Nine box priors and three scales are chosen via k-means clustering as before.

- **Feature extractor.** The authors used a new but significantly larger network called Darknet-53. Darknet-53, compared to Darknet-19, adopts residual shortcut connections (ResNet). It is proved to yield state-of-the-art results: Similar performance to ResNet-152 but $2\times$ faster. Also, Darknet-53 can better utilize the GPU.

- **Training.** They used multi-scale training instead of mining.

Then the author introduced some trials that didn't help.

- **Anchor box x,y offset predictions.** The linear activation of x,y offset as a multiple of the box width or height can decrease the stability.
- **Linear x,y predictions instead of logistic** functions lead to reduced mAP.
- **Focal loss** does not help improve the performance but drops the mAP.
- **Dual IoU thresholds and truth assignment** (Faster R-CNN) does not lead to good results.

Experiments are performed on COCO dataset. The model YOLOv3 is firstly trained on COCO trainset and then tested on the test set. Results indicate that YOLOv3 is comparable to SSD variants but $3\times$ faster, but behind the RetinaNet. However, YOLOv3 achieve state-of-the-art performance at .5IOU metric. In terms of small object detection, YOLOv3 performs better via multis-scale predictions but is worse while detecting larger objects.

### Comments

#### My thoughts

The good side is: The paper presents explicit knowledge about what factors contributes to the improvements in YOLOv3's performance. The sections are easy to follow and understand. But compared to the paper of YOLOv2, this paper is less rigorous and precise. 

- The authors revealed that he took the tables of performances of backbones from other research instead of reproducing and experimenting them uniformly.
- The authors didn't present all trials they made that failed but the things that they can remember. We may lack information of some potential but crucial issues that leads to poor performance.

#### Comparison

The paper in particular demonstrates the results of the performance at .5IOU metric. The best YOLOv3 model achieved state-of-the-art trade-off between accuracy and speed. It achieves 57.9 $AP_{50}$ in 51 ms on a Titan X, in contrast to 57.5 $AP_{50}$ in 198ms by RetinaNet. YOLOv3 was still not the most accurate detector but the most balanced and the fastest detector at that time.

#### Applications

Similar to YOLOv2, YOLOv3 can be used as the state-of-the-art detector at high FPS in high resolutions. However, in low resolutions or the cased when top accuracy is not necessary, YOLOv2 is much better because Darknet-19 can run at 171 FPS tested with $256\times 256$ input images compared to 78 by Darknet-53. The authors, in particular, addressed this issue, and they demonstrated that distinguishing an IOU of 0.3 from 0.5 is hard for humans. So for machines, .5IOU metric is also enough in object detection.